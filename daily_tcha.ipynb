{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e2505a-822c-4654-b355-a82b1335e82a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing...\n",
      "Input file: E:\\python_env\\Incident Ticket_20260131105108.xlsx\n",
      "Lookup file: SubRootcuz.xlsx\n",
      "Output file: E:\\python_env\\result.xlsx\n",
      "Target date: 20251227\n",
      "Reading data from E:\\python_env\\Incident Ticket_20260131105108.xlsx...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yahya.f\\AppData\\Local\\anaconda3\\Lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 2252 rows and 34 columns\n",
      "Target date: 2025-12-27 00:00:00 to 2025-12-27 23:59:59\n",
      "Adjusted 473 'Fault First Occur Time' entries to 2025-12-27 00:00:00\n",
      "Adjusted 428 entries for being after target date\n",
      "Adjusted 87 entries for being NaN\n",
      "MTTR calculated for 2252 rows\n",
      "MTTR_sec calculated for 2252 rows\n",
      "\n",
      "Filling empty Aggregated_RC and Aggregated_SC values...\n",
      "Filled 0 empty Aggregated_RC values with 'BSS - Under Investigation'\n",
      "Filled 2 empty Aggregated_SC values with 'Unclear'\n",
      "\n",
      "Calculating outage columns...\n",
      "Created 2G_outage using 2G-Affected Site Count(Create TT_sitecount2g)\n",
      "Created 3G_outage using 3G-Affected Site Count(Create TT_sitecount3g)\n",
      "Created 4G_outage using 4G-Affected Site Count(Create TT_sitecount4g)\n",
      "Created TDD_outage using TDD Affected Site Count(Create TT_tddsitecount)\n",
      "Created 5G_outage using 5G-Affected Site Count(Create TT_sitecount5g)\n",
      "Removed 95 rows where Aggregated_SC is 'ER'\n",
      "Removed 1 rows with empty Site Province\n",
      "\n",
      "Loading lookup data from SubRootcuz.xlsx...\n",
      "Successfully loaded lookup table with 883 rows\n",
      "Merging with lookup data...\n",
      "Error saving file: [Errno 13] Permission denied: 'E:\\\\python_env\\\\result.xlsx'\n",
      "\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "target_date_str = \"20251227\"\n",
    "input_file = \"E:\\\\python_env\\\\Incident Ticket_20260131105108.xlsx\"\n",
    "lookup_file = \"SubRootcuz.xlsx\"\n",
    "output_file = \"E:\\\\python_env\\\\result.xlsx\"\n",
    "\n",
    "def process_ticket_data_no_drop(input_file, target_date_str):\n",
    "    \"\"\"\n",
    "    Process ticket data without dropping any columns.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to input Excel file\n",
    "        target_date_str (str): Target date in format YYYYMMDD\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Processed dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Read data from Excel file\n",
    "    print(f\"Reading data from {input_file}...\")\n",
    "    try:\n",
    "        df = pd.read_excel(input_file)\n",
    "        print(f\"Successfully loaded {len(df)} rows and {len(df.columns)} columns\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Parse target date\n",
    "    try:\n",
    "        target_date = datetime.strptime(target_date_str, \"%Y%m%d\")\n",
    "        target_date_start = target_date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "        target_date_end = target_date.replace(hour=23, minute=59, second=59, microsecond=0)\n",
    "        print(f\"Target date: {target_date_start} to {target_date_end}\")\n",
    "    except ValueError:\n",
    "        print(f\"Error: Invalid date format. Expected YYYYMMDD, got {target_date_str}\")\n",
    "        return None\n",
    "    \n",
    "    # 3. Process Fault First Occur Time\n",
    "    if \"Fault First Occur Time\" in df.columns:\n",
    "        # Convert to datetime if needed\n",
    "        df[\"Fault First Occur Time\"] = pd.to_datetime(df[\"Fault First Occur Time\"], errors='coerce')\n",
    "        \n",
    "        # Apply the rule: if before target date, set to 00:00:00 of target date\n",
    "        mask_before = df[\"Fault First Occur Time\"] < target_date_start\n",
    "        df.loc[mask_before, \"Fault First Occur Time\"] = target_date_start\n",
    "        \n",
    "        print(f\"Adjusted {mask_before.sum()} 'Fault First Occur Time' entries to {target_date_start}\")\n",
    "    else:\n",
    "        print(\"Warning: 'Fault First Occur Time' column not found\")\n",
    "    \n",
    "    # 4. Process Fault Recovery Time\n",
    "    if \"Fault Recovery Time(Process TT_faultrecoverytime)\" in df.columns:\n",
    "        # Convert to datetime\n",
    "        df[\"Fault Recovery Time(Process TT_faultrecoverytime)\"] = pd.to_datetime(\n",
    "            df[\"Fault Recovery Time(Process TT_faultrecoverytime)\"], errors='coerce'\n",
    "        )\n",
    "        \n",
    "        # Apply the rule: if after target date or NaN, set to 23:59:59 of target date\n",
    "        mask_after = df[\"Fault Recovery Time(Process TT_faultrecoverytime)\"] > target_date_end\n",
    "        mask_nan = df[\"Fault Recovery Time(Process TT_faultrecoverytime)\"].isna()\n",
    "        \n",
    "        df.loc[mask_after | mask_nan, \"Fault Recovery Time(Process TT_faultrecoverytime)\"] = target_date_end\n",
    "        \n",
    "        print(f\"Adjusted {mask_after.sum()} entries for being after target date\")\n",
    "        print(f\"Adjusted {mask_nan.sum()} entries for being NaN\")\n",
    "    else:\n",
    "        print(\"Warning: 'Fault Recovery Time(Process TT_faultrecoverytime)' column not found\")\n",
    "    \n",
    "    # 5. Calculate MTTR (only if both time columns exist)\n",
    "    if \"Fault First Occur Time\" in df.columns and \"Fault Recovery Time(Process TT_faultrecoverytime)\" in df.columns:\n",
    "        df[\"MTTR\"] = df[\"Fault Recovery Time(Process TT_faultrecoverytime)\"] - df[\"Fault First Occur Time\"]\n",
    "        \n",
    "        # Cap MTTR at 23:59:59 if needed\n",
    "        max_duration = timedelta(hours=23, minutes=59, seconds=59)\n",
    "        df[\"MTTR\"] = df[\"MTTR\"].apply(lambda x: min(x, max_duration) if pd.notnull(x) else x)\n",
    "        \n",
    "        # Convert MTTR to string format HH:MM:SS\n",
    "        def format_timedelta(td):\n",
    "            if pd.isnull(td):\n",
    "                return \"00:00:00\"\n",
    "            total_seconds = int(td.total_seconds())\n",
    "            hours = total_seconds // 3600\n",
    "            minutes = (total_seconds % 3600) // 60\n",
    "            seconds = total_seconds % 60\n",
    "            return f\"{hours:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "        \n",
    "        df[\"MTTR\"] = df[\"MTTR\"].apply(format_timedelta)\n",
    "        print(f\"MTTR calculated for {len(df)} rows\")\n",
    "        \n",
    "        # Calculate MTTR_sec (MTTR in seconds)\n",
    "        def timedelta_to_seconds(td):\n",
    "            if pd.isnull(td):\n",
    "                return 0\n",
    "            return int(td.total_seconds())\n",
    "        \n",
    "        # Create a timedelta version for calculation\n",
    "        df[\"MTTR_timedelta\"] = pd.to_timedelta(df[\"MTTR\"])\n",
    "        df[\"MTTR_sec\"] = df[\"MTTR_timedelta\"].apply(timedelta_to_seconds)\n",
    "        print(f\"MTTR_sec calculated for {len(df)} rows\")\n",
    "        \n",
    "        # Drop the intermediate column\n",
    "        df = df.drop(columns=[\"MTTR_timedelta\"])\n",
    "    else:\n",
    "        print(\"Warning: Cannot calculate MTTR - missing time columns\")\n",
    "        df[\"MTTR\"] = \"00:00:00\"\n",
    "        df[\"MTTR_sec\"] = 0\n",
    "    \n",
    "    # 6. Create Aggregated_RC column\n",
    "    if \"Root Cause(Process TT_root_cause)\" in df.columns and \"Probable Cause(Process TT_possiblecause)\" in df.columns:\n",
    "        df[\"Aggregated_RC\"] = df.apply(\n",
    "            lambda row: row[\"Root Cause(Process TT_root_cause)\"] \n",
    "            if pd.notnull(row[\"Root Cause(Process TT_root_cause)\"]) \n",
    "            else row[\"Probable Cause(Process TT_possiblecause)\"],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: Required columns for Aggregated_RC not found\")\n",
    "        df[\"Aggregated_RC\"] = \"\"\n",
    "    \n",
    "    # 7. Create Aggregated_SC column\n",
    "    if \"Sub Root Cause(Process TT_sub_root_cause)\" in df.columns and \"Probable Sub Cause(Process TT_probsubcause)\" in df.columns:\n",
    "        df[\"Aggregated_SC\"] = df.apply(\n",
    "            lambda row: row[\"Sub Root Cause(Process TT_sub_root_cause)\"] \n",
    "            if pd.notnull(row[\"Sub Root Cause(Process TT_sub_root_cause)\"]) \n",
    "            else row[\"Probable Sub Cause(Process TT_probsubcause)\"],\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        print(\"Warning: Required columns for Aggregated_SC not found\")\n",
    "        df[\"Aggregated_SC\"] = \"\"\n",
    "    \n",
    "    # 7.5 Fill empty Aggregated_RC and Aggregated_SC values\n",
    "    print(\"\\nFilling empty Aggregated_RC and Aggregated_SC values...\")\n",
    "    \n",
    "    # Function to check if a value is empty (NaN, None, or empty string)\n",
    "    def is_empty(value):\n",
    "        return pd.isna(value) or value is None or str(value).strip() == \"\"\n",
    "    \n",
    "    # Fill empty Aggregated_RC with \"BSS - Under Investigation\"\n",
    "    empty_rc_count = df[\"Aggregated_RC\"].apply(is_empty).sum()\n",
    "    df[\"Aggregated_RC\"] = df[\"Aggregated_RC\"].apply(\n",
    "        lambda x: \"BSS - Under Investigation\" if is_empty(x) else x\n",
    "    )\n",
    "    print(f\"Filled {empty_rc_count} empty Aggregated_RC values with 'BSS - Under Investigation'\")\n",
    "    \n",
    "    # Fill empty Aggregated_SC with \"Unclear\"\n",
    "    empty_sc_count = df[\"Aggregated_SC\"].apply(is_empty).sum()\n",
    "    df[\"Aggregated_SC\"] = df[\"Aggregated_SC\"].apply(\n",
    "        lambda x: \"Unclear\" if is_empty(x) else x\n",
    "    )\n",
    "    print(f\"Filled {empty_sc_count} empty Aggregated_SC values with 'Unclear'\")\n",
    "    \n",
    "    # 8. Calculate outage columns\n",
    "    print(\"\\nCalculating outage columns...\")\n",
    "    \n",
    "    # Define column name mappings\n",
    "    column_mappings = {\n",
    "        \"2G_outage\": \"2G-Affected Site Count(Create TT_sitecount2g)\",\n",
    "        \"3G_outage\": \"3G-Affected Site Count(Create TT_sitecount3g)\",\n",
    "        \"4G_outage\": \"4G-Affected Site Count(Create TT_sitecount4g)\",\n",
    "        \"TDD_outage\": \"TDD Affected Site Count(Create TT_tddsitecount)\",\n",
    "        \"5G_outage\": \"5G-Affected Site Count(Create TT_sitecount5g)\"\n",
    "    }\n",
    "    \n",
    "    # Calculate each outage column\n",
    "    for outage_col, count_col in column_mappings.items():\n",
    "        if count_col in df.columns:\n",
    "            # Convert to numeric, filling NaN with 0\n",
    "            df[count_col] = pd.to_numeric(df[count_col], errors='coerce').fillna(0).astype(int)\n",
    "            df[outage_col] = df[\"MTTR_sec\"] * df[count_col]\n",
    "            print(f\"Created {outage_col} using {count_col}\")\n",
    "        else:\n",
    "            # If count column doesn't exist, set outage to 0\n",
    "            df[outage_col] = 0\n",
    "            print(f\"Warning: {count_col} not found. Setting {outage_col} to 0\")\n",
    "    \n",
    "    # 9. Drop rows where Aggregated_SC is exactly \"ER\"\n",
    "    initial_count = len(df)\n",
    "    if \"Aggregated_SC\" in df.columns:\n",
    "        # Convert to string for comparison, then check for exact match with \"ER\"\n",
    "        df = df[df[\"Aggregated_SC\"].astype(str).str.strip() != \"ER\"]\n",
    "        removed_er_count = initial_count - len(df)\n",
    "        print(f\"Removed {removed_er_count} rows where Aggregated_SC is 'ER'\")\n",
    "    else:\n",
    "        print(\"Warning: 'Aggregated_SC' column not found, cannot filter by 'ER'\")\n",
    "    \n",
    "    # 10. Drop rows where Site Province is empty (if column exists)\n",
    "    if \"Site Province(Create TT_province)\" in df.columns:\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=[\"Site Province(Create TT_province)\"])\n",
    "        df = df[df[\"Site Province(Create TT_province)\"].astype(str).str.strip() != \"\"]\n",
    "        removed_province_count = initial_count - len(df)\n",
    "        print(f\"Removed {removed_province_count} rows with empty Site Province\")\n",
    "    else:\n",
    "        print(\"Warning: 'Site Province(Create TT_province)' column not found\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main processing\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting processing...\")\n",
    "    print(f\"Input file: {input_file}\")\n",
    "    print(f\"Lookup file: {lookup_file}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    print(f\"Target date: {target_date_str}\")\n",
    "    \n",
    "    # Process the ticket data\n",
    "    df_result = process_ticket_data_no_drop(input_file, target_date_str)\n",
    "    \n",
    "    if df_result is not None:\n",
    "        # Load lookup dataframe\n",
    "        print(f\"\\nLoading lookup data from {lookup_file}...\")\n",
    "        try:\n",
    "            lookup_df = pd.read_excel(lookup_file, sheet_name='Sheet1')\n",
    "            print(f\"Successfully loaded lookup table with {len(lookup_df)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading lookup file: {e}\")\n",
    "            # If lookup fails, just use the processed df as final\n",
    "            df_final = df_result\n",
    "        else:\n",
    "            # Merge df_result with lookup_df\n",
    "            print(\"Merging with lookup data...\")\n",
    "            merged_df = pd.merge(\n",
    "                df_result,\n",
    "                lookup_df[['RC', 'SC', 'CATEGORY', 'MS/NON MS']],\n",
    "                left_on=['Aggregated_RC', 'Aggregated_SC'],\n",
    "                right_on=['RC', 'SC'],\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Drop the redundant RC and SC columns from the lookup\n",
    "            merged_df = merged_df.drop(columns=['RC', 'SC'])\n",
    "            df_final = merged_df\n",
    "        \n",
    "        # Reorder columns to have new columns at the end for easier viewing\n",
    "        all_columns = df_final.columns.tolist()\n",
    "        new_columns = [\n",
    "            \"MTTR\", \"MTTR_sec\", \"Aggregated_RC\", \"Aggregated_SC\",\n",
    "            \"2G_outage\", \"3G_outage\", \"4G_outage\", \"TDD_outage\", \"5G_outage\"\n",
    "        ]\n",
    "        \n",
    "        # Add CATEGORY and MS/NON MS if they exist\n",
    "        if \"CATEGORY\" in df_final.columns:\n",
    "            new_columns.append(\"CATEGORY\")\n",
    "        if \"MS/NON MS\" in df_final.columns:\n",
    "            new_columns.append(\"MS/NON MS\")\n",
    "        \n",
    "        other_columns = [col for col in all_columns if col not in new_columns]\n",
    "        final_columns = other_columns + new_columns\n",
    "        df_final = df_final[final_columns]\n",
    "        \n",
    "        # Save to Excel\n",
    "        try:\n",
    "            df_final.to_excel(output_file, index=False)\n",
    "            print(f\"\\nSuccessfully saved processed data to {output_file}\")\n",
    "            print(f\"Final dataset shape: {df_final.shape}\")\n",
    "            print(f\"Total columns: {len(df_final.columns)}\")\n",
    "            \n",
    "            # Show summary statistics\n",
    "            print(\"\\nSummary statistics for new columns:\")\n",
    "            if \"MTTR_sec\" in df_final.columns:\n",
    "                print(f\"MTTR_sec - Min: {df_final['MTTR_sec'].min()}s, Max: {df_final['MTTR_sec'].max()}s, Avg: {df_final['MTTR_sec'].mean():.2f}s\")\n",
    "            \n",
    "            # Show outage column totals\n",
    "            outage_columns = [\"2G_outage\", \"3G_outage\", \"4G_outage\", \"TDD_outage\", \"5G_outage\"]\n",
    "            for col in outage_columns:\n",
    "                if col in df_final.columns:\n",
    "                    total = df_final[col].sum()\n",
    "                    print(f\"{col}: Total = {total:,} site-seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file: {e}\")\n",
    "    \n",
    "    print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18fb130f-5411-4342-ab11-7355518b1e10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final processed summary DataFrames:\n",
      "\n",
      "2G Summary (7 rows):\n",
      "                         Metric  2025-12-27\n",
      "                        2G_TCHA       98.56\n",
      "      2G_outage_power_exclusion       99.39\n",
      "        2G_outage_NOA_exclusion       98.56\n",
      "2G_outage_Non_Telecom_exclusion       98.75\n",
      "    2G_outage_PA_/_UA_exclusion       98.66\n",
      "      2G_outage_Spare_exclusion       98.56\n",
      "2G_outage_Third_Party_exclusion       98.64\n",
      "--------------------------------------------------\n",
      "\n",
      "3G Summary (7 rows):\n",
      "                         Metric  2025-12-27\n",
      "                        3G_TCHA       96.56\n",
      "      3G_outage_power_exclusion       98.62\n",
      "        3G_outage_NOA_exclusion       96.56\n",
      "3G_outage_Non_Telecom_exclusion       97.00\n",
      "    3G_outage_PA_/_UA_exclusion       96.79\n",
      "      3G_outage_Spare_exclusion       96.56\n",
      "3G_outage_Third_Party_exclusion       96.73\n",
      "--------------------------------------------------\n",
      "\n",
      "4G Summary (7 rows):\n",
      "                         Metric  2025-12-27\n",
      "                        4G_TCHA       98.05\n",
      "      4G_outage_power_exclusion       99.21\n",
      "        4G_outage_NOA_exclusion       98.05\n",
      "4G_outage_Non_Telecom_exclusion       98.31\n",
      "    4G_outage_PA_/_UA_exclusion       98.16\n",
      "      4G_outage_Spare_exclusion       98.05\n",
      "4G_outage_Third_Party_exclusion       98.14\n",
      "--------------------------------------------------\n",
      "\n",
      "TDD Summary (7 rows):\n",
      "                          Metric  2025-12-27\n",
      "                        TDD_TCHA       90.00\n",
      "      TDD_outage_power_exclusion       93.42\n",
      "        TDD_outage_NOA_exclusion       90.00\n",
      "TDD_outage_Non_Telecom_exclusion       92.17\n",
      "    TDD_outage_PA_/_UA_exclusion       92.18\n",
      "      TDD_outage_Spare_exclusion       90.00\n",
      "TDD_outage_Third_Party_exclusion       90.20\n",
      "--------------------------------------------------\n",
      "\n",
      "5G Summary (7 rows):\n",
      "                         Metric  2025-12-27\n",
      "                        5G_TCHA       98.16\n",
      "      5G_outage_power_exclusion       98.82\n",
      "        5G_outage_NOA_exclusion       98.16\n",
      "5G_outage_Non_Telecom_exclusion       98.64\n",
      "    5G_outage_PA_/_UA_exclusion       98.55\n",
      "      5G_outage_Spare_exclusion       98.16\n",
      "5G_outage_Third_Party_exclusion       98.16\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# NEW PART: TCHA & OUTAGE SUMMARY DATAFRAMES\n",
    "# =========================\n",
    "\n",
    "# User-defined TCHA values\n",
    "TCHA_VALUES = {\n",
    "    \"2G\": 98.56,\n",
    "    \"3G\": 96.56,\n",
    "    \"4G\": 98.05,\n",
    "    \"5G\": 98.16,\n",
    "    \"TDD\": 90.00\n",
    "}\n",
    "\n",
    "# Categories to aggregate\n",
    "CATEGORIES = [\"power\", \"NOA\", \"Non Telecom\", \"PA / UA\", \"Spare\", \"Third Party\"]\n",
    "\n",
    "# Technology to outage column mapping\n",
    "TECH_OUTAGE_COLUMNS = {\n",
    "    \"2G\": \"2G_outage\",\n",
    "    \"3G\": \"3G_outage\",\n",
    "    \"4G\": \"4G_outage\",\n",
    "    \"TDD\": \"TDD_outage\",\n",
    "    \"5G\": \"5G_outage\"\n",
    "}\n",
    "\n",
    "def build_final_tech_summary(tech, outage_col):\n",
    "    \"\"\"\n",
    "    Build final technology summary with exclusion column and proper formatting.\n",
    "    \n",
    "    Args:\n",
    "        tech (str): Technology name (e.g., \"2G\", \"3G\", etc.)\n",
    "        outage_col (str): Outage column name in df_final\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Final technology summary DataFrame\n",
    "    \"\"\"\n",
    "    summary_rows = []\n",
    "    \n",
    "    # Total outage (will be removed later)\n",
    "    total_outage = df_final[outage_col].sum()\n",
    "    summary_rows.append({\n",
    "        \"Metric\": f\"{tech}_outage_total\",\n",
    "        \"Value\": total_outage\n",
    "    })\n",
    "    \n",
    "    # Outage per category\n",
    "    category_outages = {}\n",
    "    for category in CATEGORIES:\n",
    "        category_value = df_final.loc[\n",
    "            df_final[\"CATEGORY\"].astype(str).str.strip().str.lower() == category.lower(),\n",
    "            outage_col\n",
    "        ].sum()\n",
    "        category_outages[category] = category_value\n",
    "        summary_rows.append({\n",
    "            \"Metric\": f\"{tech}_outage_{category.replace(' ', '_')}\",\n",
    "            \"Value\": category_value\n",
    "        })\n",
    "    \n",
    "    # TCHA value\n",
    "    tcha_value = TCHA_VALUES[tech]\n",
    "    summary_rows.append({\n",
    "        \"Metric\": f\"{tech}_TCHA\",\n",
    "        \"Value\": tcha_value\n",
    "    })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(summary_rows)\n",
    "    \n",
    "    # Add exclusion column and initialize with NaN\n",
    "    df['exclusion'] = None\n",
    "    \n",
    "    # Calculate exclusion for each category\n",
    "    for category in CATEGORIES:\n",
    "        category_key = category.replace(' ', '_')\n",
    "        metric_name = f'{tech}_outage_{category_key}'\n",
    "        \n",
    "        # Find the row for this category\n",
    "        mask = df['Metric'] == metric_name\n",
    "        \n",
    "        if mask.any():\n",
    "            category_outage = category_outages[category]\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if total_outage > 0:\n",
    "                exclusion_value = tcha_value + (category_outage * (100 - tcha_value) / total_outage)\n",
    "                # Round to 2 decimal places\n",
    "                exclusion_value = round(exclusion_value, 2)\n",
    "            else:\n",
    "                exclusion_value = tcha_value\n",
    "            \n",
    "            df.loc[mask, 'exclusion'] = exclusion_value\n",
    "    \n",
    "    # Remove the total row (e.g., \"2G_outage_total\", \"3G_outage_total\", etc.)\n",
    "    df = df[~df['Metric'].str.contains('_outage_total$')].copy()\n",
    "    \n",
    "    # Fill empty exclusion values with Value column data (for TCHA row)\n",
    "    df['exclusion'] = df.apply(\n",
    "        lambda row: row['Value'] if pd.isna(row['exclusion']) else row['exclusion'],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Format the date for column renaming\n",
    "    try:\n",
    "        report_date = datetime.strptime(target_date_str, \"%Y%m%d\").strftime(\"%Y-%m-%d\")\n",
    "    except:\n",
    "        report_date = target_date_str  # fallback to original format\n",
    "    \n",
    "    # Rename exclusion column to report date\n",
    "    df = df.rename(columns={'exclusion': report_date})\n",
    "    \n",
    "    # Drop the Value column\n",
    "    df = df.drop(columns=['Value'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create final DataFrames per technology\n",
    "df_2G_outage_summary  = build_final_tech_summary(\"2G\",  TECH_OUTAGE_COLUMNS[\"2G\"])\n",
    "df_3G_outage_summary  = build_final_tech_summary(\"3G\",  TECH_OUTAGE_COLUMNS[\"3G\"])\n",
    "df_4G_outage_summary  = build_final_tech_summary(\"4G\",  TECH_OUTAGE_COLUMNS[\"4G\"])\n",
    "df_TDD_outage_summary = build_final_tech_summary(\"TDD\", TECH_OUTAGE_COLUMNS[\"TDD\"])\n",
    "df_5G_outage_summary  = build_final_tech_summary(\"5G\",  TECH_OUTAGE_COLUMNS[\"5G\"])\n",
    "\n",
    "# Process each DataFrame: put TCHA row at top and append \"_exclusion\" to category rows\n",
    "def process_tech_dataframe(df, tech):\n",
    "    \"\"\"\n",
    "    Process technology DataFrame to:\n",
    "    1. Put TCHA row at the top\n",
    "    2. Append \"_exclusion\" to category rows in Metric column\n",
    "    3. Return processed DataFrame\n",
    "    \"\"\"\n",
    "    # Get the TCHA row\n",
    "    tcha_mask = df['Metric'] == f'{tech}_TCHA'\n",
    "    tcha_row = df[tcha_mask]\n",
    "    \n",
    "    # Get category rows (all other rows)\n",
    "    category_rows = df[~tcha_mask].copy()\n",
    "    \n",
    "    # Append \"_exclusion\" to category rows' Metric values\n",
    "    category_rows['Metric'] = category_rows['Metric'] + '_exclusion'\n",
    "    \n",
    "    # Format the date for column renaming (already done in build_final_tech_summary)\n",
    "    # Get report date from column name (the column that's not 'Metric')\n",
    "    value_column = [col for col in df.columns if col != 'Metric'][0]\n",
    "    \n",
    "    # Reorder: TCHA row first, then category rows\n",
    "    processed_df = pd.concat([tcha_row, category_rows], ignore_index=True)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Process all technology DataFrames\n",
    "df_2G_outage_summary  = process_tech_dataframe(df_2G_outage_summary, \"2G\")\n",
    "df_3G_outage_summary  = process_tech_dataframe(df_3G_outage_summary, \"3G\")\n",
    "df_4G_outage_summary  = process_tech_dataframe(df_4G_outage_summary, \"4G\")\n",
    "df_TDD_outage_summary = process_tech_dataframe(df_TDD_outage_summary, \"TDD\")\n",
    "df_5G_outage_summary  = process_tech_dataframe(df_5G_outage_summary, \"5G\")\n",
    "\n",
    "# Print summary to verify\n",
    "print(\"\\nFinal processed summary DataFrames:\")\n",
    "for tech_df, tech_name in [\n",
    "    (df_2G_outage_summary, \"2G\"),\n",
    "    (df_3G_outage_summary, \"3G\"),\n",
    "    (df_4G_outage_summary, \"4G\"),\n",
    "    (df_TDD_outage_summary, \"TDD\"),\n",
    "    (df_5G_outage_summary, \"5G\")\n",
    "]:\n",
    "    print(f\"\\n{tech_name} Summary ({len(tech_df)} rows):\")\n",
    "    print(tech_df.to_string(index=False))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a699014-0619-4663-9f32-385d32ec3083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
